\chapter{Methodology}
\label{ch:Methodology}
This paper proposes a forecasting method that utilizes dimensionality reduction and clustering techniques to group days that exhibit similarity in terms of electric consumption behavior. Days that are grouped into the same cluster are thought to contain shared features, whether those features be temporal or meteorological or otherwise, that cause this similarity in behavior. The formed clusters (per household) are used for 2 purposes: firstly, they will be used to train a classification model that utilizes available context information to assign a new day to the correct cluster. Secondly, and finally, a novel deep learning method will be applied on a per-cluster basis to forecast future energy consumption. A detailed outline of the proposed model, first introduced in Section \ref{sec:Introduction:Propose-Model},  can be seen in Figure \ref{fig:Proposed-Model}.

\begin{figure}[H]
    \begin{adjustwidth}{-3.0cm}{-3.0cm}%
        \centering
        \includegraphics[width=\linewidth]{Images/Chapter 5/Other/Proposed-Model.pdf}
        \caption{Proposed daily profile extraction and load forecasting model.}
        \label{fig:Proposed-Model}
    \end{adjustwidth}
\end{figure}

\noindent \newline In short, we start off by resampling the data present in both the \gls{refit} data set as well as the \gls{ucid} into a common resolution in order be able to directly compare results. In this case we will be resampling both data sets to a common resolution of 15 minutes per sample as this lines up well with the native resolution of the meteorological data we have on hand that was provided to us by Solcast. Following this, we clean both data sets and rid them of any days that contain an incomplete number of records (incomplete here referring to any days that contain less than 96 records given we split each day into a total of 96 chunks). After this, we take a subset of each of our data sets (60\% of the total data for each of the \gls{refit} as well as the \gls{ucid} data sets henceforth referred to as \textit{Set A}) and leave out the remaining 40\% (henceforth referred to as \textit{Set B}) in order to validate the results of our forecasting model. We then reduce the overall dimensionality of a single day, going from a total of 96 features to a much more manageable 2 through a combination of statistical and machine learning techniques and generate clusters based on the new, 2-dimensional data set by utilizing a density-based clustering algorithm. Following this, we train and optimize a classifier on Set A and use it to generate cluster labels for the previously withheld Set B. Finally, we train our forecasting models, one per cluster, on the data present in Set A and use the data present in Set B to act as both a validation set as well as a training set with which to obtain final results. In this Chapter, we will provide a working example alongside in-depth explanations of each of the steps previously discussed that make up our overall forecasting pipeline.

\section{Stage 1 - Data Collection and Cleaning}
\label{sec:Methodology:Stage-1}
\begin{adjustwidth}{-1.65cm}{0.0cm}%
    \begin{enumerate}[label=Step 1.\arabic*:, leftmargin=*]
    \setlength\itemsep{1em}
        \item As mentioned previously, this paper utilizes available historical data with regards to energy consumption on an individual household basis. In reality, as part of stage 1 of our forecasting pipeline, time series data of daily electricity consumption would need to be collected from an individual household meter for an adequate amount of time at an ideal resolution so as to obtain acceptable results.
        \item After collecting, or in our case loading in, the data, we perform common preprocessing techniques to account for noisy or otherwise missing data that occurred during the transmission of the data from the meters. In our case, the available data was resampled into a resolution of 15 minutes and any days that contained less than 96 values (given that there are 96 15 minute chunks in a day) were dropped from our data set. All other days that contained \gls{nan} values were also not considered and subsequently dropped from our data set.
    \end{enumerate}
\end{adjustwidth}

\section{Stage 2 - Dimensionality Reduction and Clustering}
\label{sec:Methodology:Stage-2}
\begin{adjustwidth}{-1.65cm}{-0.0cm}%
    \begin{enumerate}[label=Step 2.\arabic*:, leftmargin=*]
    \setlength\itemsep{1em}
        \item Given that each day in our data set is represented by 96 dimensions, each dimension comprising mean active power consumption over a time period of 15 minutes, the first logical step to undertake would be to transform the data in a manner that enables our clustering techniques to more efficiently determine which days exhibit similarity in terms of electric consumption behavior. This \textit{dimensionality reduction} step comprises 2 parts that are outlined in the sub-steps below.
        
        \noindent \newline To start things off we divide each day into 5 different periods (as per the work of \citet{Yildiz}) as follows:
            \begin{enumerate}[label=\arabic*:, leftmargin=.75cm]
                \item Morning: 06:00 - 11:00
                \item Late morning/afternoon: 11:00 - 15:00
                \item Late afternoon/early evening: 15:00 - 20:30
                \item Evening: 20:30 - 23:30
                \item Late evening/early morning: 23:30 - 06:00
            \end{enumerate}
        
        \noindent \newline Following that, we represent each period by its respective mean, minimum, maximum value as well as its standard deviation. The outcome of performing this is that each day is now represented by a total of 20 dimensions rather than the initial 96 which is a reduction of $\sim 80\%$. We can reduce this even further, and even visualize our data in 2 or 3 dimensions, by making use of either of the \gls{t-sne} or \gls{umap} algorithms outlined in Section \ref{subsec:Background-Information:Dimension-Reduction:t-Distributed-Stochastic-Neighbor-Embedding}. The most important hyperparameter to tune for either algorithm is the \textit{perplexity} hyperparameter for the \gls{t-sne} algorithm and the equivalent $n_{\text{neighbors}}$ hyperparameter for the \gls{umap} algorithm. During our research, we found that an optimal value for either of these hyperparameters is $N^{\frac{1}{2}}$ where N is the number of samples present in the data set. To better understand each of the steps of our proposed model, we will now begin a series of visualizations showcasing each step as performed on the \gls{ucid} data set throughout the entirety of stage 2 as well as the remainder of the stages that make up our overall forecasting pipeline. We start off by presenting a scatter plot of the 2-dimensional output obtained as a result of performing the \gls{t-sne} algorithm which can be seen in Figure \ref{fig:UCID-t-SNE} that allows us to clearly visualize the 2-dimensional interpretation of the samples present in the \gls{ucid} data set. Each of the points found on the 2-dimensional surface in Figure \ref{fig:UCID-t-SNE} represents a single day, and given that the \gls{umap} algorithm claims to preserve both local as well as most of the global structure present in the data we can safely assume that distances between the samples are conducive to the similarity in terms of energy consumption as per the previously segmented (into various periods) interpretation of the data.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-t-SNE.pdf}
            \caption{The output of performing the \gls{umap} algorithm on the 20-dimensional \gls{ucid} data set. Each point in this figure represents a single sample (or day) within our data set mapped onto a 2-dimensional surface.}
            \label{fig:UCID-t-SNE}
        \end{figure}
        
        \item After performing the dimensionality reduction step on our data, we proceed to cluster the resulting output by applying the \gls{hdbscan} algorithm, as outlined in Section \ref{subsec:Background-Information:Clustering-Algorithms:HDBSCAN}. As previously mentioned, the only important parameters that need to be passed to the \gls{hdbscan} algorithm are the minimum size we expect each cluster to be. In this case we set that value to $\frac{1}{10}\left(N\right)$ where N is the number of samples present in the data set. Our reasoning for selecting this value is predominantly based on the adequate results observed by \citet{Kong} in their implementation of the \gls{dbscan} algorithm in a similar setting whilst utilizing a similar selection in terms of hyperparameter settings. The other hyperparameter we choose to tune is the \textit{min\_samples} hyperparameter, which, in layman's terms, denotes how conservative we would like to be with our clustering in terms of restricting clusters to progressively more dense areas and classifying samples from our data set as noise. In our case, an arbitrary value of 15 was selected, in contrast to the default value that sets $min\_samples = min\_cluster\_size$. The results of performing the \gls{hdbscan} algorithm on our 2-dimensional representation of the \gls{ucid} data set (shown in Figure \ref{fig:UCID-t-SNE}) can be seen in Figure \ref{fig:UCID-HDBSCAN-1}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-HDBSCAN-1.pdf}
            \caption{The output of performing the \gls{hdbscan} algorithm on the 2-dimensional \gls{ucid} data set previously seen in Figure \ref{fig:UCID-t-SNE}.}
            \label{fig:UCID-HDBSCAN-1}
        \end{figure}
        
        \noindent \newline For the sake of comparison, we present the output of applying the k-means clustering algorithm (assuming $k = 3$) on the same 2-dimensional representation of the \gls{ucid} data set. This can be seen in Figure \ref{fig:UCID-K-Means}. We note immediately the capability of the \gls{hdbscan} algorithm in capturing a better representation of the clusters present in our 2-dimensional representation of the \gls{ucid} data set. The representation of outliers as noise points and not having to have a priori knowledge on the number of clusters present in the data we are working with is a definite pro as well further compounding our choice of clustering algorithm in our proposed model.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-K-Means.pdf}
            \caption{The output of performing the k-means algorithm on the 2-dimensional \gls{ucid} data set previously seen in Figure \ref{fig:UCID-t-SNE}.}
            \label{fig:UCID-K-Means}
        \end{figure}
        
        \item Visualizing, or otherwise manually inspecting, the clusters we obtain as a result of our application of the \gls{hdbscan} algorithm is necessary so that we can better understand whether our clustering algorithm truly captures the habits of the individuals residing in the households we are working with. The first step in our analysis of the resulting clusters would be to plot the averaged power consumption on a per cluster basis so that we may be able to clearly visualize the patterns in power consumption per cluster. An example of this, in line with the previous examples showcasing our proposed model on the \gls{ucid} data set, can be seen in Figure \ref{fig:UCID-HDBSCAN-2}. We note that, in this example, a subset of our data (24 samples in total), were recorded as noise by the \gls{hdbscan} algorithm. Inspecting these samples manually lead us to the confirmation that, of the 4 year's worth of data, these 24 days were the only days that exhibited no tangible shift in terms of power consumption throughout the entirety of the day (\ie the global active power draw observed was completely stationary throughout this period); however, this is not explicitly outlined in the documentation of the \gls{ucid} data set. This can be seen as a more or less flat line in Figure \ref{fig:UCID-HDBSCAN-2}.
    
        \begin{figure}[hbt!]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-HDBSCAN-2.pdf}
            \caption{Average power consumption per hour of the day for each of the resulting clusters obtained after utilizing the \gls{hdbscan} algorithm on our 2-dimensional representation of the \gls{ucid} data set.}
            \label{fig:UCID-HDBSCAN-2}
        \end{figure}
        
        \noindent \newline Following this, Figures \ref{fig:UCID-HDBSCAN-3} and \ref{fig:UCID-HDBSCAN-4} help us visualize the distribution of the clusters over the months of the year as well as the days of the week to ascertain whether any of the clusters present any correlation with these temporal variables. Given that the initial spread of the data throughout the months of the year and days of the week of the \gls{ucid} were relatively uniform, we should not see any bias towards any particular month or day in either Figure \ref{fig:UCID-HDBSCAN-3} or Figure \ref{fig:UCID-HDBSCAN-4} respectively.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-HDBSCAN-3.pdf}
            \caption{Distribution of the clusters over the different months of the year.}
            \label{fig:UCID-HDBSCAN-3}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-HDBSCAN-4.pdf}
            \caption{Distribution of the clusters over the different days of the week.}
            \label{fig:UCID-HDBSCAN-4}
        \end{figure}
        
        \noindent \newline At a glance, we notice that clusters 1 and 2 are more likely to occur on the weekdays with cluster 3 taking over the majority share of the weekend which tends to explain the more consistent draw in power throughout the entirety of the day for samples that belong to cluster 3. Furthermore, samples in cluster 1 tend to gravitate towards the warmer, summer months peaking in terms of number of occurrences in the month of July while samples in clusters 2 and 3 exhibit a more uniform spread over the remainder of the colder months which could explain the lower average draw in power present in samples that belong to cluster 1 being a result of the owners of the home not being in as often or potentially not needing to make use of appliances to heat up their home (we note that this data was collected in Sceaux, France that experiences a warm season of $\sim \! 3$  months with otherwise, generally, cooler temperatures). 
        
        \begin{figure}[hbt!]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 2/UCID/UCID-HDBSCAN-5.pdf}
            \caption{Spread in number of samples per cluster label.}
            \label{fig:UCID-HDBSCAN-5}
        \end{figure}
        
        \noindent \newline \textit{N.B.: It is worth noting that performing these same steps on households from within the \gls{refit} data set exhibit similar results.}
    \end{enumerate}
\end{adjustwidth}

\section{Stage 3 - Further Data Preprocessing}
\label{sec:Methodology:Stage-3}
\begin{adjustwidth}{-1.65cm}{-0.0cm}%
    \begin{enumerate}[label=Step 3.\arabic*:, leftmargin=*]
    \item The details pertaining to the majority of the steps undertaken throughout the entirety of stage 3 of the proposed model have, to an extent, been explained in-depth during the \gls{eda} performed in Chapter \ref{ch:Exploratory-Data-Analysis}. Nonetheless, a brief summary will be provided as part of Chapter \ref{ch:Methodology}. The first step undertaken, again on a per-cluster basis, is to append both temporal data as well as meteorological data to our data sets. Table \ref{tab:Temporal-variables} pertains to the temporal variables that will be taken into consideration as part of this feature engineering step while Table \ref{tab:Solcast-parameters} pertains to the obtained, historical meteorological data that concern the regions associated with our data sets. Incidentally, as outlined in Table \ref{tab:Temporal-variables}, the temporal variables we have chosen to append do not hold much value given their current format. This is due mostly in part to their cyclical nature (think of how the 23rd hour of the day is rather close to hours 0 and 1). To handle this we can encode our temporal variables (for example, through the use of both the sine and cosine function) in an attempt to transpose our linear interpretation of time into a cyclical state that can be better interpreted by our deep learning model further down the line. The result of performing this so-called encoding can be seen in both Figures \ref{fig:Temporal-Illustration-1} and \ref{fig:Temporal-Illustration-2}.
    
    \begin{figure}[H]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Representing the time of the day as a combination of both sine and cosine waves. ]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/Temporal_Illustration/Temporal_Illustration_1.pdf}
                        \label{fig:Temporal-Illustration-1}
                } \quad
                \subfloat[Visualizing our cyclical encoding of the time of day.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/Temporal_Illustration/Temporal_Illustration_2.pdf}
                        \label{fig:Temporal-Illustration-2}
                } \quad
                \caption{By utilizing a combination of the sine function and the cosine function, we eliminate the possibility that two different times would receive the same value had we used either function independently. The combination of both functions can be thought of as an artificial 2-axis coordinate system that represents the time of day.}
        \end{adjustwidth}
    \end{figure}
    
    \item Following the feature engineering process, the feature selection process, heavily revolves around minimizing the overall number of features that do not serve as good predictors of our target variable. This has been explored predominantly in Section \ref{sec:Exploratory-Data-Analysis:Causality-and-Correlation}. To summarize on the steps undertaken in the sections mentioned prior -- we performed a series of tests to determine whether our variables (temporal or meteorological) present a significant level of independent (or combinatorial) correlation or causation against our target variable for each of the \gls{refit} and \gls{ucid} data sets. The primary tests conducted revolved around the concepts of Granger Causality and mutual information gain although other factors (such as a per-variable variance threshold) were also looked into.
    
    \item When taking our target variable into consideration, the notion of outliers (and how to deal with them), is inevitable. Scaling the values in such a fashion that accounts for outliers is one possibility whilst defining a threshold and trimming outlier values is another possibility. Alternatively, leaving them in is another possibility as some level of noise is unavoidable in the data collection process and training our models on unrealistically curated data does not serve to produce an accurate representation of a real-life scenario in which a model of this caliber could be applied. Nonetheless, we explore a few possibilities with respect to dealing with outlier values. One possibility, assuming a Gaussian distribution of the values of our target variable, would be to remove all values that fall a pre-defined number of standard deviations, generally 2 or 3, away from the mean. Unfortunately, the distribution of our target variable does not fall under this pre-condition (as seen in Figure \ref{fig:UCID-GAP-Distribution}) and so this is not a feasible option. Another method, explored during prior, related research \cite{Kareem}, worked on the basis of defining an upper and lower bound based on the \gls{iqr}. The \gls{iqr} is calculated as the difference between the 75th (Q3) and 25th (Q1) percentiles of the data and comprises the box in a traditional box and whiskers plot. Using the \gls{iqr} we can define outliers as any values that are a pre-defined factor below the 25th percentile or above the 75th percentile as follows:
    
    \begin{equation}
        Q1 - (1.5 * IQR) < x < Q3 + (1.5 * IQR)
    \label{eq:IQR-Outliers}
    \end{equation}
    
    \noindent \newline Figures \ref{fig:UCID-Box-and-Whiskers-GAP-Out} and \ref{fig:UCID-Box-and-Whiskers-GAP-NoOut} represent the distribution of values for our target variable over the different months of the year both before and after removing outliers respectively.

    
    \begin{figure}[hbt!]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Before removing outliers.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/UCID/UCID-Box-and-Whiskers-GAP-Out.pdf}
                        \label{fig:UCID-Box-and-Whiskers-GAP-Out}
                } \quad
                \subfloat[After removing outliers.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/UCID/UCID-Box-and-Whiskers-GAP-NoOut.pdf}
                        \label{fig:UCID-Box-and-Whiskers-GAP-NoOut}
                } \quad
                \caption{Illustrating the distribution of values with respect to the global active power of the \gls{ucid} data set both before and after removing outlier values as defined by Equation \ref{eq:IQR-Outliers}.}
        \end{adjustwidth}
    \end{figure}
    
    \noindent \newline \textit{Smoothing}, or otherwise filtering, the data can also be done through the use of a variety of techniques and can help alleviate some of the issues inherent to the noise present in our data as a byproduct of the data collection process. An example of performing a preliminary smoothing step on energy consumption data can be seen in the work of \citet{Hsiao} in which a moving (or rolling) average method was utilized. With regards to our proposed forecasting pipeline, we will be utilizing Savitzky-Golay filters \cite{Savitzky} to smooth our raw, electrical energy consumption data as, when compared to the moving average method, Savitzky-Golay filters tend to do a better job at preserving the integrity of the raw data.  Figures \ref{fig:UCID-Smoothing-Rolling-Average} and \ref{fig:UCID-Smoothing-Savitzy-Golay-Filter} serve to illustrate the application of both the moving average method as well as the Savitzky-Golay filter method on a subset of our (raw) data set in order to better visualize the differences between both methods.
    
    \begin{figure}[hbt!]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Application of the moving average method with a window size of 3.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/UCID/UCID-Smoothing-Rolling-Average.pdf}
                        \label{fig:UCID-Smoothing-Rolling-Average}
                } \quad
                \subfloat[Application of the Savitzky-Golay filter method with a polynomial order of 3 and a window size of 5.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 3/UCID/UCID-Smoothing-Savitzky-Golay-Filter.pdf}
                        \label{fig:UCID-Smoothing-Savitzy-Golay-Filter}
                } \quad
                \caption{Illustrating the application of both the moving average method as well the Savitzky-Golay filter method in smoothing on a subset of our raw data.}
        \end{adjustwidth}
    \end{figure}
    
    \noindent \newline Similarily, when considering the trend component of our data (as seen in Figure \ref{fig:REFIT-House-12-Time-Series-Decomposition}); a preliminary smoothing step can be undertaken through the use of \gls{loess} -- this is illustrated in Figure \ref{fig:UCID-Trend-Smoothing-LOESS}.
    
    \begin{figure}[hbt!]
        \centering
        \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 3/UCID/UCID-Trend-Smoothing-LOESS.pdf}
        \caption{An illustration of the previously obtained trend component both with and without the application of \gls{loess}.}
        \label{fig:UCID-Trend-Smoothing-LOESS}
    \end{figure}
    
    \item The final step taken as part of Stage 3 of the forecasting pipeline is to split the data into 3 subsets that serve to act as training, validation and testing sets that will be fed to both our classification tree as well as the CNN-LSTM network that we will be using for the purpose of forecasting. A split employing an arbitrarily selected ratio of 60:20:20 is taken. Given the nature of our study, we choose not to shuffle the data in either of the generated sets as we are primarily interested in our model's capability of forecasting future trends in energy consumption given a measure of historically available data.
    \end{enumerate}
\end{adjustwidth}

\section{Stage 4 - Training and Testing}
\label{sec:Methodology:Stage-4}
In contrast to the earlier stages, stage 4 will be subdivided into Subsections \ref{subsec:Methodology:Stage-4:Classification-Tree} and \ref{subsec:Methodology:Stage-4:CNN-LSTM-Network}, where Subsection \ref{subsec:Methodology:Stage-4:Classification-Tree} serves to present an overview of our classification model while Subsection \ref{subsec:Methodology:Stage-4:CNN-LSTM-Network} serves to present an overview of our forecasting model.

\subsection{Stage 4.1 - Classification Tree}
\label{subsec:Methodology:Stage-4:Classification-Tree}
Before we can begin attempting to forecast trends in energy consumption we will need to establish, or otherwise ascertain, our ability to correctly assign a new point (or day) to the \textit{correct} cluster. Given that the previously discussed clustering step separated the days in our data set on the basis of similarity in terms of patterns in energy consumption; this will not be an easy feat as the remaining, available context information may not suffice in providing the relevant information to draw up a decision boundary (of sorts) that serves to differentiate individual clusters.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/Other/SMOTE.pdf}
    \caption{An illustration of the \gls{smote} algorithm in the case of 2 classes depicted by blue squares (minority class) and red circles (majority class). The blue square on the far left is isolated from other members of its class and is surrounded by members of the other class and is this considered to be a noise point. The cluster in the center contains several blue squares surrounded by members from the other class and thus is indicative of potentially \textit{unsafe} points that are unlikely to be random noise. Finally, the cluster in the far right contains predominantly isolated blue squares. The algorithm would then generate new, synthetic samples prioritizing the safer regions.}
    \label{fig:SMOTE}
\end{figure}

\noindent \newline The first step in insuring a decently trained classifier is to deal with the glaring problem of class imbalance that can be seen in Figure \ref{fig:UCID-HDBSCAN-5}. The results of our clustering step lead us to an uneven distribution of days among the different class labels which could lead to poor predictive performance as standard classification algorithms are inherently biased to the majority class. A common means to alleviate this issue is to either undersample the majority class(es) or oversample the minority class(es). In this paper we will be implementing the \gls{smote} algorithm, a form of informed oversampling, that works on the basis of generalizing the decision region for minority classes and thus provides us with synthetic samples while preventing overfitting. For further explanations as to the workings, advantages as well as shortcomings of this algorithm we refer the reader to the initial paper by \citet{Chawla} as well as Figure \ref{fig:SMOTE} that provides a layman's explanation of the algorithm. The results of applying the \gls{smote} algorithm, and the overall negation of the previously mentioned class imbalance, can be seen in Figure \ref{fig:UCID-SMOTENC}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-SMOTENC.pdf}
    \caption{Number of samples per class label after applying the \gls{smote} algorithm.}
    \label{fig:UCID-SMOTENC}
\end{figure}

\noindent \newline After handling the class imbalance problem we can shift our attention to both the feature engineering as well as the feature selection process of this particular classification problem. In this scenario, the available context information we have is purely temporal (ordinal day of the week/year, month, season, etc.), and historical as well as forecasted meteorological data (air temperature, humidity, cloud opacity, etc.), as provided by Solcast, and these will serve to act as the base-line number of features that our classifier will receive with which to assign a new sample into one of the previously generated clusters. Numerous methods exist to minimize the overall amount of features being passed to our classification model, some of which we explored in Chapter \ref{ch:Background-Information} of this paper and can be reapplied here to similar effect. In brief, we chose to make use of a Random Forest Classifier, the hyperparameters of which were tuned through a randomized search over a pre-determined distribution of values per hyperparameter. After assessing the optimal hyperparameters for our use-case, we passed the model as well as the complete set of features through a feature selection algorithm titled \gls{rfecv}. \gls{rfecv} works on the basis providing a cross-validated selection of the most important features when considering a target label and pruning the less important features. Applying this algorithm reduces the overall number of features that our Random Forest Classifier utilizes from an initial 77 down to a mere 24 (as shown in Figure \ref{fig:UCID-RFECV}) which is an overall reduction of $\sim$ 68\%.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-RFECV.pdf}
    \caption{Assessing the number of important features through the use of the \gls{rfecv} algorithm. In this particular scenario, the optimal number of features was pruned down from a total of 77 to a mere 24.}
    \label{fig:UCID-RFECV}
\end{figure}

\noindent \newline After transforming the data set and pruning the less important features we, once again, train the model on the new, transformed data set utilizing 5-fold stratified cross-validation to assess whether our model is overfitting at any stage and so as to ensure an even distribution of class labels per validation set. We can conduct a quick inspection of the now fitted model by calculating the permutation feature importance on a per-feature basis to validate whether the final set of features are relevant when attempting to classify a new sample into the correct cluster. By definition, the permutation importance of a feature is the overall decrease in accuracy of our model when said feature's values are randomly shuffled and, by doing so, we break the relationship between the feature and the target label. By doing this we can assess how much our model depends on said feature, the results of which can be seen in Figure \ref{fig:UCID-Classification-Permutation-Feature-Importance}. it is important to note that, when calculating the permutation importance of strongly correlated features -- the model will still have access to the shuffled feature through its correlated feature which will result in a lower importance value for \textit{both} features when they might actually be important. To address this, it is possible to further prune the data set and remove subsets of features that present strong inter-correlation.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-Classification-Permutation-Importance.pdf}
    \caption{The permutation importance of each of the features chosen as part of our fitted Random Forest classifier.}
    \label{fig:UCID-Classification-Permutation-Feature-Importance}
\end{figure}

\noindent \newline The final model is then ready to accept new samples and assigns them a cluster based on the training procedure outlined through Section \ref{subsec:Methodology:Stage-4:Classification-Tree}.

\subsection{Stage 4.2 - CNN-LSTM Network}
\label{subsec:Methodology:Stage-4:CNN-LSTM-Network}
The focal point of our research lies in the implementation of  a \gls{cnn-lstm} model in which the \gls{cnn} component serves to learn the relative importance of each of the features (cyclical, temporal as well as meteorological) that we pass to the network as input in what we can loosely call a \textit{feature extraction} step. The extracted features are then passed to the \gls{lstm} portion of the network that learns the temporal relationship between past, or otherwise historical, values of said features with the present, or future, value(s) of the target variable where finally, an output prediction is made. The combination of both \gls{cnn} and \gls{lstm} components allows the network to learn spatio-temporal relationships between the features being passed as input and the target variable that we are attempting to forecast. In contrast to other architectures and forecasting models, this architecture is demonstrably more efficient \cite{Kim} when tackling time series problems such as that of residential energy consumption forecasting. The sample network illustrated in Figure \ref{fig:UCID-CNN-LSTM-Model} can be expanded to forecast multiple time steps ahead with minor adjustments and is capable of understanding patterns at variable time resolutions. For the purposes of this example, we will be moving forward with the previously defined resolution of 15 minutes using a window of 24 historical values $(t - 24, t - 23, ..., t)$ to make a prediction one step into the future $(t + 1)$ for both the previously established trend component as well as the raw, unadulterated data.

\begin{figure}[hbt!]
    \begin{adjustwidth}{-1.5cm}{-1.5cm}%
        \centering
        \includegraphics[width=\linewidth]{Images/Chapter 5/Stage 4/UCID/UCID-CNN-LSTM-Model-2.pdf}
        \caption{A simple, example \gls{cnn-lstm} network that makes one-step-ahead predictions.}
        \label{fig:UCID-CNN-LSTM-Model}
    \end{adjustwidth}
\end{figure}

\noindent \newline To train our network, we will be utilizing Adam \cite{Kingma}: an adaptive learning rate optimization algorithm that was designed specifically for training deep neural networks. In contrast to the ever-familiar \gls{sgd}, Adam leverages the power of adaptive learning rate methods and momentum to allocate individual learning rates for each parameter of the network being trained. For further explanations as to the workings of this algorithm we refer the reader to the initial paper by \citet{Kingma}. Additionally, when training our network(s), we will be making use of a variety of techniques to improve generalization and prevent overfitting to the training data set(s). The first of these techniques is the notion of \textit{early stopping} (illustrated in Figure \ref{fig:Early-Stopping}). Early stopping is a form of regularization that monitors the validation loss (or generalization error) and aborts the training when the monitored values either begin to degrade or do not shift for an arbitrarily set number of epochs. The second technique we will be using works on the notion of employing a variable learning rate that, in theory, facilitates convergence of our weight update rule and prevents learning from stagnating thus allowing us to break through plateaus and avoid settling at local minima.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-CNN-LSTM-Training-Validation-Loss.pdf}
%     \caption{Training and validation loss when applying the previously defined network (illustrated in Figure \ref{fig:UCID-CNN-LSTM-Model}) on the raw \gls{ucid} data set in an attempt to make predictions one time step into the future.}
%     \label{fig:UCID-CNN-LSTM-Training-Validation-Loss}
% \end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/Other/Early-Stopping.pdf}
    \caption{Illustration of early stopping.}
    \label{fig:Early-Stopping}
\end{figure}

\noindent \newline For the purposes of our experiments and procuring the results showcased in Chapter \ref{ch:Results-and-Discussion}, we will be implementing a network on a per-cluster basis for both the raw data as well as the trend component of each of our data sets. The networks implemented will serve to provide one-step-ahead forecasts as well as one-shot 12-step-ahead (3 hour) forecasts as proof of concept.

% \begin{figure}[H]
%     \begin{adjustwidth}{-3.0cm}{-3.0cm}%
%             \myfloatalign
%             \subfloat[An example of where our network performs optimally, achieving a \gls{mape} of $\sim 11\%$.]
%             {
%                     \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-Forecast-1-Step-Ahead-Good-Example.pdf}
%                     \label{fig:UCID-Forecast-1-Step-Ahead-Good-Example}
%             } \quad
%             \subfloat[An example of where our network performs sub-optimally, achieving a \gls{mape} of $\sim 26\%$.]
%             {
%                     \includegraphics[width=0.7\textwidth]{Images/Chapter 5/Stage 4/UCID/UCID-Forecast-1-Step-Ahead-Poor-Example.pdf}
%                     \label{fig:UCID-Forecast-1-Step-Ahead-Poor-Example}
%             } \quad
%             \caption{An illustration of one-step-ahead forecasts on 2 separate days in an attempt to showcase cases in which our network performs both optimally and sub-optimally.}
%     \end{adjustwidth}
% \end{figure}